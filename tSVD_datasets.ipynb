{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tSVD import calculate_rr, error_func_frobenius\n",
    "from common import construct_adjacency_matrix\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_rr(data, error_func=error_func_frobenius):\n",
    "    results = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        A = construct_adjacency_matrix(data[i])\n",
    "        t, rr, errors = calculate_rr(A, error_func)\n",
    "        results.append(\n",
    "            {\n",
    "                \"graph_id\": i,\n",
    "                \"n_nodes\": data[i].x.shape[0],\n",
    "                \"rr\": rr,\n",
    "                \"errors\": errors,\n",
    "                \"time\": t\n",
    "            }\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZINC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tolsi/miniconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "/home/tolsi/miniconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "/home/tolsi/miniconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/io/fs.py:215: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import ZINC\n",
    "\n",
    "# subset=True selects the small version of the dataset\n",
    "# the split parameter chooses between the test/train/validation sets\n",
    "# for the SVD analysis its probably best to just use train as its the largest.\n",
    "data_zinc = ZINC(subset=True, root='data', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:16<00:00, 50.80it/s]\n"
     ]
    }
   ],
   "source": [
    "zinc_results = find_all_rr(data_zinc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zinc_results).to_parquet('results/ZINC/updated.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tolsi/miniconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "/home/tolsi/miniconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "/home/tolsi/miniconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/io/fs.py:215: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import LRGBDataset\n",
    "\n",
    "# here we have the name parameter because LRGB has multiple benchmarks.\n",
    "# for now lets focus on peptides-func\n",
    "data_peptides = LRGBDataset(name='Peptides-func', root='data', split='train')\n",
    "\n",
    "# G = nx.Graph()\n",
    "# #edges for the first graph  in the dataset (index 0)\n",
    "# edges = zip(*d[0].edge_index.tolist())\n",
    "# G.add_edges_from(edges)\n",
    "# nx.draw(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10873/10873 [10:14:25<00:00,  3.39s/it]   \n"
     ]
    }
   ],
   "source": [
    "peptides_results = find_all_rr(data_peptides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(peptides_results).to_parquet('results/peptides_updated.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import GNNBenchmarkDataset\n",
    "\n",
    "# we want specifically the CIFAR10 benchmark from this dataset\n",
    "data_cifar = GNNBenchmarkDataset(name='CIFAR10', root='data', split='train')\n",
    "\n",
    "# G = nx.Graph()\n",
    "# #edges for the first graph  in the dataset (index 0)\n",
    "# edges = zip(*d[0].edge_index.tolist())\n",
    "# G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45000/45000 [7:14:10<00:00,  1.73it/s]   \n"
     ]
    }
   ],
   "source": [
    "cifar_results = find_all_rr(data_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cifar_results).to_parquet('results/cifar_updated.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cifar_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>graph_id</th>\n",
       "      <th>n_nodes</th>\n",
       "      <th>rr</th>\n",
       "      <th>errors</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>68</td>\n",
       "      <td>[1.0, 1.0, 0.9925861538059417, 0.9717603520509...</td>\n",
       "      <td>0.943575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>117</td>\n",
       "      <td>70</td>\n",
       "      <td>[1.0, 1.0, 0.994643775085452, 0.97785304683095...</td>\n",
       "      <td>0.800947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>114</td>\n",
       "      <td>59</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.9795048874617234, 0.97332852...</td>\n",
       "      <td>0.286410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>114</td>\n",
       "      <td>63</td>\n",
       "      <td>[1.0, 0.9994516040168878, 0.9994516040168878, ...</td>\n",
       "      <td>1.007908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>118</td>\n",
       "      <td>62</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.9845200155491758, 0.96055686...</td>\n",
       "      <td>1.034828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44995</th>\n",
       "      <td>44995</td>\n",
       "      <td>117</td>\n",
       "      <td>75</td>\n",
       "      <td>[1.0, 1.0, 0.996789718842316, 0.96686558519220...</td>\n",
       "      <td>0.601059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44996</th>\n",
       "      <td>44996</td>\n",
       "      <td>119</td>\n",
       "      <td>64</td>\n",
       "      <td>[1.0, 1.0, 0.9936774919374964, 0.9750026933813...</td>\n",
       "      <td>0.359792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44997</th>\n",
       "      <td>44997</td>\n",
       "      <td>117</td>\n",
       "      <td>63</td>\n",
       "      <td>[1.0, 1.0, 0.9924931914339438, 0.9635449151019...</td>\n",
       "      <td>0.803766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44998</th>\n",
       "      <td>44998</td>\n",
       "      <td>123</td>\n",
       "      <td>68</td>\n",
       "      <td>[1.0, 1.0, 1.0, 0.9830887114773246, 0.96482864...</td>\n",
       "      <td>0.433006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44999</th>\n",
       "      <td>44999</td>\n",
       "      <td>120</td>\n",
       "      <td>67</td>\n",
       "      <td>[1.0, 0.9989577902327338, 0.9984362773857929, ...</td>\n",
       "      <td>0.441190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       graph_id  n_nodes  rr  \\\n",
       "0             0      110  68   \n",
       "1             1      117  70   \n",
       "2             2      114  59   \n",
       "3             3      114  63   \n",
       "4             4      118  62   \n",
       "...         ...      ...  ..   \n",
       "44995     44995      117  75   \n",
       "44996     44996      119  64   \n",
       "44997     44997      117  63   \n",
       "44998     44998      123  68   \n",
       "44999     44999      120  67   \n",
       "\n",
       "                                                  errors      time  \n",
       "0      [1.0, 1.0, 0.9925861538059417, 0.9717603520509...  0.943575  \n",
       "1      [1.0, 1.0, 0.994643775085452, 0.97785304683095...  0.800947  \n",
       "2      [1.0, 1.0, 1.0, 0.9795048874617234, 0.97332852...  0.286410  \n",
       "3      [1.0, 0.9994516040168878, 0.9994516040168878, ...  1.007908  \n",
       "4      [1.0, 1.0, 1.0, 0.9845200155491758, 0.96055686...  1.034828  \n",
       "...                                                  ...       ...  \n",
       "44995  [1.0, 1.0, 0.996789718842316, 0.96686558519220...  0.601059  \n",
       "44996  [1.0, 1.0, 0.9936774919374964, 0.9750026933813...  0.359792  \n",
       "44997  [1.0, 1.0, 0.9924931914339438, 0.9635449151019...  0.803766  \n",
       "44998  [1.0, 1.0, 1.0, 0.9830887114773246, 0.96482864...  0.433006  \n",
       "44999  [1.0, 0.9989577902327338, 0.9984362773857929, ...  0.441190  \n",
       "\n",
       "[45000 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet('results/cifar_updated.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
